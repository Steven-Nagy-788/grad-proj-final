# 1.6 Project Planning and Management

This section outlines the implementation timeline, resource allocation, and budget planning for the automated game level testing framework. The project follows an iterative development approach with clearly defined milestones and deliverables.

## 1.6.1 Project Timeline Revisited

### Timeline Overview
The project is structured as a **12-week (3-month) implementation cycle** following the documentation phase, divided into four major phases: Foundation (Weeks 1-3), Core Backend Development (Weeks 4-7), Frontend & Integration (Weeks 8-10), and Testing & Finalization (Weeks 11-12).

### Team Structure (5 Members)

| Role | Team Member | Primary Responsibilities |
|------|-------------|--------------------------|
| **Team Lead & Backend Developer** | Member 1 | Database architecture, REST API development, integration coordination |
| **RL/ML Engineer** | Member 2 | Agent integration, telemetry extraction, reproducibility testing |
| **AI Engineer (CV/LLM)** | Member 3 | Computer Vision model training, LLM integration, bug detection algorithms |
| **Frontend Developer** | Member 4 | Web dashboard (starts Week 7), data visualization components, UI/UX design |
| **QA & DevOps Engineer** | Member 5 | Testing pipeline, deployment automation, documentation, performance optimization |

### Phase Breakdown (Day-by-Day Schedule)

---

## Phase 1: Foundation & Setup (Weeks 1-3, Days 1-21)

### Week 1 (Days 1-7)

**Day 1-2: Environment Setup & Project Initialization**
- Member 5: Install VizDoom, PyTorch, PostgreSQL on all machines
- Member 5: Setup Git repository, branching strategy, CI/CD skeleton
- Member 1: Initialize database project structure (migrations, ORM setup)
- Member 2: Clone pre-trained DQN agent repository, verify dependencies
- Member 3: Research CV architectures (ResNet, EfficientNet comparison)
- Member 4: Research frontend stack options (React vs Vue), create decision document
- **Deliverable:** All development environments operational, repo structure created

**Day 3-4: Database Schema Implementation**
- Member 1: Design and implement core tables (maps, runs, events, metrics)
- Member 1: Add foreign key constraints, indexes on frequently queried columns
- Member 1: Write migration scripts and seed data for testing
- Member 2: Write test script to load agent model and initialize VizDoom
- Member 3: Begin annotating training dataset (100 frames, 5 bug categories)
- Member 5: Setup PostgreSQL backup automation
- **Deliverable:** Database schema complete with migration scripts, agent loads successfully

**Day 5-7: Basic Test Runner Prototype**
- Member 2: Implement single-map test runner (load map, run agent, capture outcome)
- Member 2: Add seed control for reproducibility
- Member 1: Create database insertion functions for test runs
- Member 3: Continue dataset annotation (200 frames completed)
- Member 4: Study existing map visualization libraries (Plotly, D3.js)
- Member 5: Write unit tests for database models
- **Milestone (Day 7):** Agent successfully completes 1 map, basic data stored in database

---

### Week 2 (Days 8-14)

**Day 8-10: Telemetry Extraction Module**
- Member 2: Implement frame-by-frame data capture (health, ammo, position, events)
- Member 2: Create event logging system with timestamp precision
- Member 1: Optimize database insertion (batch inserts, transaction handling)
- Member 3: Finish dataset annotation (500 frames, 7 bug categories completed)
- Member 5: Setup automated testing pipeline (pytest integration)
- **Deliverable:** 10,500 events captured per 5-episode run, stored in <2 seconds

**Day 11-14: Reproducibility Validation & Data Pipeline**
- Member 2: Run reproducibility tests (5 identical-seed runs, measure variance)
- Member 2: Implement crash detection and graceful error handling
- Member 1: Add database query functions (get_map_stats, get_run_events)
- Member 3: Setup CV training environment (GPU instance, data loaders)
- Member 5: Write integration tests (end-to-end test run)
- Member 5: Create performance monitoring dashboard (query times, insert rates)
- **Milestone (Day 14):** <10% variance across runs verified, complete pipeline operational

---

### Week 3 (Days 15-21)

**Day 15-17: Bug Detection Algorithms (Behavioral)**
- Member 2: Implement stuck state detection (position unchanged 100+ frames)
- Member 2: Implement instant death detection (health drop >80 in 1 frame)
- Member 2: Implement unreachable area detection (coverage heatmap analysis)
- Member 3: Begin CV model training (transfer learning from ResNet-50)
- Member 1: Create bug_reports table schema
- Member 5: Write unit tests for bug detection logic
- **Deliverable:** Behavioral bug detection operational, 90%+ accuracy on test cases

**Day 18-21: Computer Vision Setup**
- Member 3: Fine-tune CV model on annotated dataset (500 frames)
- Member 3: Implement frame analysis pipeline (30 FPS processing)
- Member 3: Create visual_anomalies table integration
- Member 2: Test CV model on 5 diverse maps
- Member 1: Add API endpoint stubs (maps, runs, bugs - no implementation yet)
- Member 5: Setup GPU monitoring (utilization, memory tracking)
- **Milestone (Day 21):** CV model achieves 70%+ precision, integrated with test pipeline

---

## Phase 2: Core Backend Development (Weeks 4-7, Days 22-49)

### Week 4 (Days 22-28)

**Day 22-24: LLM Integration Foundation**
- Member 3: Setup LLM API clients (OpenAI GPT-4V, Anthropic Claude)
- Member 3: Implement prompt engineering for bug descriptions
- Member 3: Create smart sampling logic (invoke only when bugs detected)
- Member 1: Implement REST API core structure (Flask/FastAPI routes)
- Member 2: Create batch testing script (process multiple maps sequentially)
- **Deliverable:** LLM generates natural language bug reports with context

**Day 25-28: Hardness Scoring & Solvability**
- Member 2: Implement hardness score algorithm (weighted formula 0-100)
- Member 2: Implement solvability detection logic
- Member 2: Add hardness_score and is_solvable columns to metrics table
- Member 3: Optimize LLM prompts (reduce token usage by 30%)
- Member 1: Implement API endpoints: GET /api/maps, GET /api/maps/{id}/stats
- Member 5: Load testing (simulate 100 concurrent API requests)
- **Milestone (Day 28):** Hardness scoring validated on 10 maps, API returns statistics

---

### Week 5 (Days 29-35)

**Day 29-31: Advanced API Endpoints**
- Member 1: Implement GET /api/maps/{id}/bugs (return LLM descriptions with screenshots)
- Member 1: Implement GET /api/compare?maps=id1,id2 (comparative analysis)
- Member 1: Add caching layer (Redis) for frequently accessed queries
- Member 3: Train CV model on expanded dataset (750 frames)
- Member 2: Implement parallel execution support (4 concurrent test runs)
- **Deliverable:** 5 core API endpoints operational with <500ms response time

**Day 32-35: Query Optimization & Analytics**
- Member 1: Implement GET /api/timeline/{run_id} (frame-by-frame event stream)
- Member 1: Add database indexing for performance (map_id, run_id, event_type)
- Member 1: Create aggregate statistics endpoint (total maps, avg hardness, bug distribution)
- Member 3: Validate CV + LLM hybrid system on 20 maps
- Member 5: Write API integration tests (test all endpoints with mock data)
- **Milestone (Day 35):** 85%+ bug detection precision achieved, API complete

---

### Week 6 (Days 36-42)

**Day 36-38: Batch Testing Pipeline**
- Member 2: Implement batch testing CLI (process directory of maps)
- Member 2: Add progress tracking and ETA estimation
- Member 2: Implement test scheduling (queue system for large batches)
- Member 1: Add POST /api/maps/{id}/test endpoint (trigger test via API)
- Member 3: Create PDF report generation module (bug summaries)
- Member 5: Setup monitoring alerts (email notifications on failures)
- **Deliverable:** Batch testing processes 10+ maps unattended

**Day 39-42: Performance Optimization**
- Member 1: Optimize database queries (reduce N+1 queries, add eager loading)
- Member 1: Implement pagination for large result sets
- Member 2: Profile agent execution (identify bottlenecks)
- Member 3: Optimize CV model inference (batch frame processing)
- Member 5: Run performance benchmarks (test 50 maps, measure metrics)
- **Milestone (Day 42):** System tests 20+ maps in <2 hours, all backend complete

---

### Week 7 (Days 43-49)

**Day 43-45: API Documentation & Stabilization**
- Member 1: Generate OpenAPI/Swagger documentation
- Member 1: Add API authentication (optional JWT tokens)
- Member 1: Write API usage examples and tutorials
- Member 2: Fix edge cases (very large maps, corrupt files, timeouts)
- Member 3: Create bug report templates (JSON, PDF formats)
- Member 5: Security audit (SQL injection prevention, input validation)
- **Deliverable:** Complete API documentation with examples

**Day 46-49: Backend Integration Testing**
- Member 5: End-to-end testing (30+ diverse maps)
- Member 5: Stress testing (100 maps, measure system limits)
- Member 2: Bug fixes from integration testing
- Member 3: Fine-tune LLM prompts based on testing feedback
- Member 1: Database migration for production schema
- **Member 4: BEGINS FRONTEND WORK** (project scaffolding, initial setup)
- **Milestone (Day 49):** Backend 100% complete, 30+ maps tested successfully

---

## Phase 3: Frontend & Integration (Weeks 8-10, Days 50-70)

### Week 8 (Days 50-56)

**Day 50-52: Frontend Project Setup**
- Member 4: Initialize React.js project (Create React App or Vite)
- Member 4: Setup routing (React Router), state management (Redux/Context)
- Member 4: Create project structure (components, pages, services)
- Member 4: Implement API client (Axios with base URL config)
- Member 1: Add CORS headers to API for local frontend development
- Member 5: Setup frontend CI/CD pipeline (build, lint, deploy)
- **Deliverable:** Frontend dev server running, API connection verified

**Day 53-56: Landing Page & Map Library**
- Member 4: Design and implement landing page (hero section, quick stats)
- Member 4: Create map library grid view (thumbnails, hardness badges)
- Member 4: Implement map list API integration (fetch and display maps)
- Member 4: Add search and filter functionality (by difficulty, solvability)
- Member 1: Optimize map list endpoint (add pagination, thumbnails)
- Member 5: Frontend testing setup (Jest, React Testing Library)
- **Deliverable:** Map library page operational with filtering

---

### Week 9 (Days 57-63)

**Day 57-59: Map Detail Page (Part 1)**
- Member 4: Create map detail page layout (overview section)
- Member 4: Display hardness score (gauge/dial visualization)
- Member 4: Show solvability indicator and basic stats (runs, avg time)
- Member 4: Implement navigation from map list to detail page
- Member 1: Add map thumbnail generation endpoint
- Member 2: Support Member 4 with test data generation
- **Deliverable:** Map detail page shows basic statistics

**Day 60-63: Bug Report Dashboard**
- Member 4: Design bug report list component (table with filters)
- Member 4: Display LLM-generated descriptions with severity badges
- Member 4: Add screenshot modal (click to view full frame)
- Member 4: Implement frame number links (jump to timeline)
- Member 3: Optimize screenshot delivery (compress images, lazy loading)
- Member 5: Frontend performance testing (Lighthouse audit)
- **Deliverable:** Bug reports displayed with natural language descriptions

---

### Week 10 (Days 64-70)

**Day 64-66: Timeline Visualization (Part 1)**
- Member 4: Create timeline component (Plotly.js/D3.js)
- Member 4: Display telemetry graphs (health, ammo over time)
- Member 4: Add event markers (deaths, pickups) on timeline
- Member 4: Implement zoom and pan controls
- Member 1: Optimize timeline API (reduce payload size)
- **Deliverable:** Interactive timeline shows telemetry data

**Day 67-70: Timeline Visualization (Part 2) & Comparative View**
- Member 4: Add video playback synchronized with timeline
- Member 4: Overlay bug markers with LLM descriptions on timeline
- Member 4: Create comparative analysis page (side-by-side maps)
- Member 4: Implement radar chart for difficulty dimensions
- Member 1: Support Member 4 with compare API troubleshooting
- Member 5: Cross-browser testing (Chrome, Firefox, Safari)
- **Milestone (Day 70):** Timeline playback operational, comparison view complete

---

## Phase 4: Testing & Finalization (Weeks 11-12, Days 71-84)

### Week 11 (Days 71-77)

**Day 71-73: Frontend-Backend Integration**
- Member 1 & Member 4: Pair programming for integration issues
- Member 4: Fix API request errors, loading states, error handling
- Member 4: Add loading spinners and skeleton screens
- Member 4: Implement toast notifications for user feedback
- Member 5: End-to-end testing (user flows from map upload to report)
- **Deliverable:** Complete user workflows tested and functional

**Day 74-77: Advanced Visualizations & Polish**
- Member 4: Create bug occurrence heatmaps
- Member 4: Add analytics dashboard (aggregate statistics, charts)
- Member 4: Implement dark mode toggle
- Member 4: Polish UI (responsive design, mobile support)
- Member 3: Generate 50+ test reports for demo data
- Member 5: Accessibility audit (WCAG compliance check)
- **Deliverable:** Frontend polished with advanced visualizations

---

### Week 12 (Days 78-84)

**Day 78-80: System-Wide Testing**
- Member 5: User acceptance testing (simulate real-world workflows)
- Member 5: Performance testing (50+ maps, measure frontend load times)
- Member 2: Bug fixes from UAT
- Member 4: Frontend bug fixes (UI inconsistencies, edge cases)
- Member 1: Database optimization (vacuum, reindex)
- **Deliverable:** All critical bugs fixed, system stable

**Day 81-82: Documentation Finalization**
- Member 5: Write user guide (step-by-step tutorials with screenshots)
- Member 5: Write deployment guide (Docker setup, production config)
- Member 2: Update architecture documentation (add diagrams)
- Member 1: Finalize API documentation with examples
- Member 4: Create frontend component documentation
- **Deliverable:** Complete documentation package

**Day 83-84: Presentation & Release Preparation**
- All Members: Prepare final presentation (slides, demo script)
- All Members: Record demo video (5-minute walkthrough)
- Member 1: Create GitHub release (tag version, release notes)
- Member 5: Deploy to production environment (cloud hosting)
- All Members: Final rehearsal and Q&A preparation
- **Milestone (Day 84):** PROJECT COMPLETE - System operational, documented, and ready for demonstration

---

## Timeline Summary by Phase

| Phase | Duration | Key Focus | Team Members |
|-------|----------|-----------|--------------|
| **Phase 1: Foundation** | Days 1-21 (3 weeks) | Database, Agent, CV Training | Member 1, 2, 3, 5 |
| **Phase 2: Core Backend** | Days 22-49 (4 weeks) | API, LLM, Bug Detection, Testing | Member 1, 2, 3, 5 |
| **Phase 3: Frontend** | Days 50-70 (3 weeks) | Dashboard, Visualization, Integration | Member 1, 4, 5 (support) |
| **Phase 4: Finalization** | Days 71-84 (2 weeks) | Testing, Documentation, Presentation | All Members |

**Frontend Schedule:** Member 4 starts Day 46-49 (project setup during backend stabilization), works full-time Days 50-84 (5.5 weeks total)

### Task Dependencies

Critical path dependencies:
1. **Database Schema → Telemetry Extraction → API Development**
   - Database must exist before data can be stored (Member 1 → Member 2 → Member 1)
   - API queries require populated database tables

2. **Agent Integration → Bug Detection → LLM Integration**
   - Agent must generate gameplay data before bugs can be detected (Member 2 → Member 3)
   - LLM requires both telemetry and CV outputs for context (Member 3)

3. **Core Backend → Frontend Dashboard**
   - REST API must be operational before frontend can query data (Member 1 → Member 4)
   - Visualization depends on database aggregation functions

4. **All Components → Integration Testing**
   - QA testing blocked until all modules operational (Member 5 depends on all)

### GANTT Chart (12-Week Timeline)

```
┌──────────────────────────┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┐
│ Task                     │W1 │W2 │W3 │W4 │W5 │W6 │W7 │W8 │W9 │W10 │W11 │W12 │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Environment Setup (M5)   │███│   │   │   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Database Impl (M1)       │███│███│   │   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Agent Integration (M2)   │███│███│███│   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Telemetry Extract (M2)   │   │███│███│   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ CV Bug Detection (M3)    │   │   │███│███│   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ LLM Integration (M3)     │   │   │   │███│███│   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Hardness Score (M2)      │   │   │   │███│   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ REST API Dev (M1)        │   │   │   │███│███│███│   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Batch Testing (M2)       │   │   │   │   │   │███│███│   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Frontend Setup (M4)      │   │   │   │   │   │   │███│   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Frontend UI (M4)         │   │   │   │   │   │   │   │███│███│ ███│    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Visualization (M4)       │   │   │   │   │   │   │   │   │███│ ███│ ███│    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Integration Test (M5)    │   │   │   │   │   │   │   │   │   │    │ ███│ ███│
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Documentation (All)      │   │   │   │   │   │   │   │   │   │    │ ███│ ███│
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Presentation Prep (All)  │   │   │   │   │   │   │   │   │   │    │    │ ███│
└──────────────────────────┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┘

Legend: ███ = Active development phase
        M1-M5 = Team members responsible
        
Key Dates:
- Day 1-21 (W1-W3): Backend foundation
- Day 22-49 (W4-W7): Core backend development
- Day 46-49 (W7): Frontend starts (project setup)
- Day 50-70 (W8-W10): Frontend full development
- Day 71-84 (W11-W12): Testing and finalization
```

### Recommended Tools for Project Management

**GANTT Chart Creation:**

1. **GanttProject** (FREE - Highly Recommended for Students)
   - **Pros:** 
     - Completely free and open-source
     - Desktop application (Windows/Mac/Linux)
     - Professional GANTT charts with task dependencies
     - Export to PNG, PDF, HTML
     - Easy to use, no learning curve
     - **Perfect for 12-week day-by-day schedules**
   - **Cons:** 
     - No cloud sync (local files only)
     - Basic compared to enterprise tools
   - **Best For:** Graduation projects, detailed day-by-day planning
   - **Download:** ganttproject.biz

2. **Jira (Recommended for daily task management)**
   - **Pros:** 
     - Industry-standard for software projects
     - Built-in Agile/Scrum boards with sprint planning
     - Timeline view for week-by-week visualization
     - Issue tracking integrated with timeline
     - Free for teams up to 10 users
     - **Great for tracking daily tasks across 5 team members**
   - **Cons:** 
     - True GANTT requires Jira Premium ($7.75/user/month)
     - Steeper learning curve
   - **Best For:** Day-to-day sprint management, task assignments

3. **Monday.com**
   - **Pros:**
     - Intuitive drag-and-drop GANTT chart builder
     - Real-time collaboration
     - Beautiful visualizations
     - Timeline view with day-level granularity
   - **Cons:**
     - Paid only (starts at $8/user/month = $40/month for 5 users)
   - **Best For:** Teams with budget for premium tools

4. **Notion** (Alternative for Combined Planning)
   - **Pros:**
     - Free for students (verification required)
     - Combines documentation + task tracking + timeline
     - Day-by-day task tracking with calendar view
     - Collaborative workspace
   - **Cons:**
     - No native GANTT (requires templates or integrations)
   - **Best For:** All-in-one documentation and task management

5. **Mermaid Gantt Diagrams** (For Documentation)
   - **Pros:**
     - Code-based (version controllable)
     - Renders in Markdown files
     - Free, integrates with VS Code, GitHub
   - **Cons:**
     - Static (not interactive)
     - Limited day-level granularity
   - **Best For:** Embedding GANTT in documentation

**Our Recommendation for Your 5-Person, 12-Week, Day-by-Day Project:**
- **GANTT Chart for Documentation:** GanttProject (export professional PNG/PDF for Word document)
- **Daily Task Management:** Jira free tier (create tasks per day, assign to members, track in sprints)
- **Communication:** Slack/Discord free tier (daily standups, announcements)
- **Documentation:** Notion free student plan or Google Docs

**Setup Workflow:**
1. **Day 1:** Create 84 tasks in GanttProject (one per day) with dependencies
2. **Day 1:** Setup Jira board with 12 sprints (one per week)
3. **Weekly:** Break down weekly tasks into daily tickets in Jira
4. **Daily:** Team updates Jira task status (To Do → In Progress → Done)
5. **Weekly Review:** Update GanttProject milestones based on actual progress

### Risk Mitigation

| Risk | Probability | Impact | Mitigation Strategy | Owner |
|------|-------------|--------|---------------------|-------|
| CV model training takes longer than expected | Medium | High | Start with transfer learning; Member 3 begins Week 1, not Week 3 | Member 3 |
| LLM API costs exceed budget | Low | Medium | Implement smart sampling; Member 3 adds caching layer | Member 3 |
| GPU availability constraints | Medium | Medium | Reserve cloud GPU early; Member 5 coordinates access | Member 5 |
| Database query performance degrades | Low | Medium | Member 1 implements indexing from day 1; weekly perf reviews | Member 1 |
| Agent fails on unconventional maps | High | Low | Test diverse maps early (Week 2); document limitations | Member 2 |
| Frontend-backend integration conflicts | Medium | Medium | Daily standups; Member 1 & 4 pair programming in Week 6 | Member 1, 4 |
| Team member availability issues | Medium | High | Cross-training: each member understands 2+ components; documented handoff procedures | All |

### Communication Plan

**Daily Standups (15 minutes):**
- What did you complete yesterday?
- What are you working on today?
- Any blockers?

**Weekly Sprint Reviews (Friday, 1 hour):**
- Demo completed features
- Review milestone progress
- Plan next week's tasks

**Tools:**
- Slack/Discord: Daily communication
- Jira: Task tracking and sprint planning
- GitHub: Code reviews, pull requests
- Google Drive: Shared documentation

### Deviation Reporting Process

**If timeline slips:**
1. Identify blocking task and root cause (responsible member documents in Jira)
2. Team lead escalates to supervisor if milestone delivery shifts by >3 days
3. Hold emergency sprint planning to redistribute work
4. Update GANTT chart and communicate new ETA to all stakeholders

**Scope Adjustment Protocol:**
- **Must-Have (MVP):** Agent integration, telemetry extraction, database storage, basic bug detection, 2+ API endpoints
- **Should-Have:** CV module, LLM integration, hardness scoring, frontend dashboard
- **Could-Have:** PDF reports, advanced visualizations, batch testing UI

If time-constrained, drop features in reverse order (could-have → should-have), preserving MVP at all costs.

---

## 1.6.2 Preliminary Budget (Low-Budget Adjusted)

### Budget Overview

**Target Budget:** **$0-50 USD** (5-person team, 12-week development cycle)  
**Cost per Member:** $0-10 per person  
**Strategy:** Maximize free-tier services, leverage existing hardware, use open-source alternatives

---

### Itemized Cost Breakdown

| Category | Item | Unit Cost | Quantity | Total | Justification |
|----------|------|-----------|----------|-------|---------------|
| **Hardware** | | | | | |
| | Local GPU (Team-Owned) | $0 | 1+ devices | **$0** | Use existing NVIDIA GPU (GTX 1060 or better); sufficient for CV training and inference |
| | Development Machines | $0 | 5 | **$0** | Personal laptops (existing); 16GB RAM minimum, CPU inference supported |
| | Cloud GPU (Backup) | $0.26/hour | 0-20 hours | **$0-5** | AWS EC2 g4dn.xlarge spot instances (only if local GPU unavailable); reserved for emergency CV training |
| **Software & APIs** | | | | | |
| | LLM (Ollama - Local) | $0 | - | **$0** | **PRIMARY CHOICE:** Open-source LLaMA 3.2 (8B) via Ollama; runs on local GPU, zero API costs |
| | LLM (Groq API - Backup) | $0 | 100-200 calls | **$0** | **BACKUP OPTION:** Groq offers free tier for LLaMA 3 (6000 requests/day); use if local LLM insufficient |
| | LLM (OpenAI - Optional) | $0.01-0.03/call | 0-50 calls | **$0-1.50** | **OPTIONAL:** GPT-4o-mini for 10-20 complex bug reports only; budgeted if quality gap exists |
| | Database (Supabase) | $0 | - | **$0** | **PRIMARY CHOICE:** Free tier (500MB, 2GB bandwidth/month, unlimited API requests); upgrade only if exceeding limits |
| | Database (PostgreSQL Local) | $0 | - | **$0** | **BACKUP OPTION:** Local PostgreSQL on development machines if Supabase limits reached |
| | AWS S3 (Screenshots) | $0.023/GB | 0-10GB | **$0-0.23** | Store bug screenshots and video recordings; first 5GB free (new AWS accounts), $0.023/GB after |
| | VizDoom | $0 | - | **$0** | Open-source (MIT license) |
| | PyTorch | $0 | - | **$0** | Open-source (BSD license) |
| | Pre-trained DQN Agent (Arnold) | $0 | - | **$0** | Existing models from research repository |
| **Project Management** | | | | | |
| | Jira (Free Tier) | $0 | 5 users | **$0** | Up to 10 users free; sufficient for team task tracking |
| | GanttProject | $0 | - | **$0** | Open-source desktop application for Gantt charts |
| | Notion (Student Plan) | $0 | 5 users | **$0** | Free for students (.edu email); documentation and planning |
| **Data & Resources** | | | | | |
| | CV Training Dataset | $0 | 500-750 frames | **$0** | Self-annotated; team labor distributed (3-4 hours per member) |
| | Test Maps (Doom WADs) | $0 | 50+ maps | **$0** | Community-created maps from idgames archive (freely available) |
| **Development Tools** | | | | | |
| | Flask/FastAPI | $0 | - | **$0** | Open-source Python frameworks |
| | React.js (Vite) | $0 | - | **$0** | Open-source frontend library with Vite build tool |
| | Git/GitHub | $0 | - | **$0** | Free tier (public repository, unlimited collaborators) |
| | VS Code / PyCharm | $0 | - | **$0** | Free community editions |
| | Ollama (LLM Runtime) | $0 | - | **$0** | Open-source local LLM inference server |
| **Collaboration** | | | | | |
| | Discord | $0 | - | **$0** | Free tier (unlimited messages, voice channels) |
| | Google Workspace | $0 | - | **$0** | Free tier (15GB storage per member) |
| | Zoom / Google Meet | $0 | - | **$0** | Free tier (40-min meetings / unlimited 1-hour meetings) |
| **Hosting & Deployment** | | | | | |
| | Frontend Hosting (Vercel) | $0 | - | **$0** | Free tier for React apps (100GB bandwidth/month) |
| | Backend Hosting (Render) | $0 | - | **$0** | Free tier for Flask API (750 hours/month) |
| | Domain Name (Optional) | $0-12 | 0-1 | **$0-12** | Optional .dev domain via Google Domains; not required for project |
| | SSL Certificate | $0 | - | **$0** | Let's Encrypt (free) or automatic via Vercel/Render |
| **Contingency** | | | | | |
| | Emergency Cloud GPU | - | - | **$0-20** | If local GPU fails or insufficient VRAM; AWS spot instances as backup |
| | LLM API Overage | - | - | **$0-10** | If Groq free tier exhausted and quality requires OpenAI calls |
| | Database Upgrade | - | - | **$0-10** | Supabase Pro ($25/month) for 1 month only if free tier exceeded (unlikely) |
| **TOTAL (Expected)** | | | | **$0-5** | **Most likely outcome:** $0 if local resources sufficient |
| **TOTAL (Max Budget)** | | | | **$50** | **Worst-case scenario:** All contingencies triggered |

---

### Cost Optimization Strategy (Zero-Cost Priority)

#### **1. GPU Usage (Target: $0)**

**Primary Approach:**
- Use team member's local NVIDIA GPU for all CV training and LLM inference
- Training schedule: Weeks 2-4 (CV model), low-priority batch processing overnight
- Expected load: 40-60 hours total GPU time (CV training 20h, inference 30h, LLM 10h)
- **Cost:** $0

**Backup Plan (if local GPU unavailable):**
- AWS EC2 g4dn.xlarge spot instances ($0.26/hour vs $0.526 on-demand)
- Reserve 10-20 hours maximum for critical CV training only
- Batch all training into 2-3 intensive sessions (Week 3, Week 5)
- **Cost:** $2.60-5.20

**Alternative (Google Colab):**
- Colab free tier: 12 hours/day GPU (T4), use for CV training only
- Restart every 12 hours, save checkpoints frequently
- **Cost:** $0

---

#### **2. LLM Integration (Target: $0)**

**PRIMARY CHOICE: Ollama (Local LLM)**

**Setup:**
```bash
# Install Ollama on local GPU machine
curl -fsSL https://ollama.com/install.sh | sh

# Download LLaMA 3.2 8B model (4.7GB)
ollama pull llama3.2:8b

# Run local API server
ollama serve
```

**Integration:**
```python
# Member 3: LLM bug description generator
import requests

def generate_bug_description(bug_data):
    prompt = f"Analyze this game bug: {bug_data['type']} at frame {bug_data['frame']}. Health: {bug_data['health']}, Position: {bug_data['position']}. Describe in 2-3 sentences."
    
    response = requests.post('http://localhost:11434/api/generate', json={
        'model': 'llama3.2:8b',
        'prompt': prompt,
        'stream': False
    })
    return response.json()['response']
```

**Advantages:**
- Zero API costs (runs on local GPU)
- No rate limits or token costs
- Privacy: No data sent to external services
- LLaMA 3.2 8B quality: ~85% as good as GPT-3.5 for structured tasks

**Trade-offs:**
- Requires 8GB GPU VRAM (acceptable for GTX 1060 6GB with quantized model)
- Slower inference: 2-5 seconds per description (vs <1s for cloud APIs)
- Slightly lower quality than GPT-4, but sufficient for bug descriptions

**Cost:** $0

---

**BACKUP OPTION: Groq API (Free Tier)**

If local GPU insufficient for both CV + LLM:

```python
# Use Groq's free LLaMA 3 API (6000 requests/day free)
import os
from groq import Groq

client = Groq(api_key=os.environ.get("GROQ_API_KEY"))  # Free API key

def generate_bug_description_groq(bug_data):
    completion = client.chat.completions.create(
        model="llama-3.1-8b-instant",  # Free tier
        messages=[{"role": "user", "content": f"Describe this bug: {bug_data}"}],
    )
    return completion.choices[0].message.content
```

**Groq Limits:**
- 6000 requests/day free (sufficient for 200 maps × 10 bugs = 2000 descriptions)
- Fast inference (200+ tokens/second)
- **Cost:** $0

---

**OPTIONAL: OpenAI GPT-4o-mini (Paid Fallback)**

Use only for 10-20 critical bugs requiring highest quality descriptions:

- Cost: $0.01-0.03 per complex bug report
- Budget: Max 50 calls = $1.50
- **Only if:** Ollama + Groq both fail quality threshold

**Total LLM Budget:** $0-1.50 (most likely $0)

---

#### **3. Database Hosting (Target: $0)**

**PRIMARY CHOICE: Supabase (Free Tier)**

**Free Tier Limits:**
- Database Size: 500MB (sufficient for 500-1000 test runs)
- Bandwidth: 2GB/month (query API + data transfer)
- API Requests: Unlimited
- Active connections: Unlimited
- Paused after 7 days inactivity (auto-resume on access)

**Capacity Estimate:**
- 20 maps × 5 runs = 100 test runs
- 100 runs × 10,500 events × 0.5KB/event = 525MB
- **Verdict:** Free tier sufficient for MVP (20-30 maps testable)

**Setup:**
```bash
# Member 1: Initialize Supabase project
# 1. Create account at supabase.com (free)
# 2. Create new project (select free tier)
# 3. Copy connection string
# 4. Setup SQLAlchemy connection

# Example connection
DATABASE_URL = "postgresql://[user]:[password]@db.[project].supabase.co:5432/postgres"
```

**Cost:** $0

**Monitoring:**
- Member 1 checks storage usage weekly
- If approaching 400MB, implement data cleanup (delete old test runs)
- **Upgrade trigger:** Only if testing >50 maps (unlikely in graduation project scope)

---

**BACKUP OPTION: Local PostgreSQL**

If Supabase limits exceeded:

```bash
# Install PostgreSQL locally (Ubuntu)
sudo apt install postgresql postgresql-contrib

# Each team member runs local database
# Sync via SQL dumps for shared testing
pg_dump mydb > backup.sql
```

**Trade-offs:**
- Zero cost
- Requires each member to setup locally (1-2 hours initial setup)
- No centralized database (harder collaboration)

**Cost:** $0

---

#### **4. File Storage (Screenshots/Videos) (Target: $0-0.50)**

**PRIMARY CHOICE: AWS S3 (Free Tier + Pay-as-you-go)**

**AWS Free Tier (First 12 months):**
- 5GB storage free
- 20,000 GET requests, 2,000 PUT requests free per month

**Estimated Usage:**
- 200 test runs × 5 bug screenshots × 50KB/image = 50MB
- 50 video recordings × 20MB/video = 1GB
- **Total:** 1-1.5GB (well within 5GB free tier)

**Pricing Beyond Free Tier:**
- Storage: $0.023/GB/month
- 10GB storage = $0.23/month
- **Cost:** $0-0.50 over 3 months

**Alternative (if no AWS account):**

**GitHub Releases:**
- Upload screenshot archives as release assets (unlimited storage for open-source)
- Member 5: Script to upload/download via GitHub API
- **Cost:** $0

**Supabase Storage:**
- 1GB free storage included in database free tier
- Simple API, integrated with main database
- **Cost:** $0

**Recommendation:** Start with Supabase Storage (simplest), migrate to S3 if >1GB

---

#### **5. Deployment & Hosting (Target: $0)**

**Frontend (React Dashboard):**

**Vercel (Free Tier):**
- Unlimited deployments
- 100GB bandwidth/month (sufficient for 100-500 users)
- Automatic SSL, CDN, previews
- **Cost:** $0

**Setup:**
```bash
# Member 4: Deploy frontend
npm install -g vercel
vercel deploy
# Follow prompts, connect to GitHub repo
```

---

**Backend (Flask API):**

**Render (Free Tier):**
- 750 hours/month free (sufficient for 24/7 uptime)
- Auto-deploy from GitHub
- Automatic SSL
- **Limitation:** Spins down after 15 min inactivity (30s cold start)

**Cost:** $0

**Alternative (Railway):**
- $5 free credit per month
- No spin-down (always active)
- Better for demos (no cold start)
- **Cost:** $0 (free credit sufficient)

---

### Recommended Architecture (Zero-Cost Stack)

```
┌─────────────────────────────────────────────────────────┐
│                   DEPLOYMENT ARCHITECTURE               │
└─────────────────────────────────────────────────────────┘

Frontend (React)               Backend (Flask API)
├─ Hosting: Vercel (Free)     ├─ Hosting: Render (Free)
├─ CDN: Automatic             ├─ API Endpoints: REST
└─ SSL: Automatic             └─ SSL: Automatic
         │                              │
         └──────────┬───────────────────┘
                    │
                    ▼
         ┌──────────────────────┐
         │  Supabase (Free)     │
         │  - PostgreSQL DB     │
         │  - File Storage (1GB)│
         └──────────────────────┘
                    │
                    ▼
         ┌──────────────────────┐
         │  Local GPU Machine   │
         │  - CV Training       │
         │  - Ollama LLM (8B)   │
         │  - VizDoom Testing   │
         └──────────────────────┘

Total Monthly Cost: $0
```

---

### Alternative: Cloud GPU Option (If Local GPU Unavailable)

**Google Colab Pro (if needed):**
- Cost: $9.99/month
- A100 GPU access (40GB VRAM)
- 24-hour continuous runtime
- **Use case:** If no local GPU available
- **Duration:** Subscribe for 1 month only (Week 2-3 for CV training)
- **Cost:** $10 (one-time)

**AWS EC2 Spot Instances (minimal usage):**
- g4dn.xlarge: $0.26/hour (vs $0.526 on-demand)
- Reserve 10-20 hours maximum
- **Cost:** $2.60-5.20

---

### Final Budget Summary

| Scenario | Cost | Description |
|----------|------|-------------|
| **Optimal Path** | **$0** | Local GPU + Ollama LLM + Supabase free tier + Vercel/Render hosting |
| **Likely Path** | **$0-5** | Occasional AWS spot instances (10h) + S3 storage (<1GB) |
| **Contingency Path** | **$20-30** | Colab Pro (1 month) + Groq API backup + minor cloud costs |
| **Maximum Budget** | **$50** | All contingencies triggered + optional domain + extended cloud GPU |

---

### Cost Sharing Agreement

**Per-Member Contribution:**
- **Expected:** $0-1 per person (total $0-5)
- **Maximum:** $10 per person (total $50, contingency buffer)

**Payment Triggers:**
1. **Week 1:** Assess local GPU capability → If insufficient, budget $10 for Colab Pro
2. **Week 3:** Check Supabase usage → If >400MB, budget $5 for cleanup or upgrade
3. **Week 5:** Evaluate LLM quality → If Ollama insufficient, budget $2 for Groq/OpenAI
4. **Week 7:** Monitor S3 usage → If >5GB, budget $0.50 for storage

**Approval Process:**
- Team Lead (Member 1) approves all expenses >$5
- Monthly cost review: Day 30, Day 60, Day 84
- Transparency: All receipts shared in Discord channel

---

### Budget vs Manual QA Cost Comparison

**Manual Testing Baseline:**
- Junior QA Tester: $15-25/hour
- Testing Speed: 2 maps/hour (30 min per map average)
- Cost for 20 Maps: **$150-250** (10 hours labor)

**Our Automated Framework:**
- Setup Cost: $0-5 (one-time)
- Testing Speed: 10 maps/hour (6 min per map)
- Cost for 20 Maps: **$0** (after setup)
- **Savings:** $150-250 (100% cost reduction)
- **Time Savings:** 10 hours → 2 hours (80% reduction)

**ROI:** Framework pays for itself on first 20 maps tested

---

### Budget Approval

**Approved By:** Prof./Dr. Mohamed Taher (pending)  
**Funding Source:** Team-sponsored ($0-10 per member)  
**Contingency Protocol:** 
- If costs exceed $30, pause cloud spending
- Fallback to 100% local resources (PostgreSQL local, no LLM integration)
- Maintain core functionality: Agent testing + CV bug detection + database storage

**Success Criteria:**
- ✓ Complete project within $0-50 total budget
- ✓ Deliver all core features (agent, CV, database, API, dashboard)
- ✓ No individual member pays >$10

---

## 1.6.3 User Interface & Interaction Model

### System Access Points

The framework provides three primary interfaces for user interaction:

**1. Command-Line Interface (CLI) - Primary Testing Interface**
```bash
# Run single map test
python test_runner.py --map maps/arena_x1.wad --episodes 5

# Batch testing
python test_runner.py --map-dir maps/ --parallel 4

# Query results
python query_api.py --map-id arena_x1 --output json
```

**Use Case:** Automated CI/CD integration, scripted testing pipelines, power users

**2. REST API - Programmatic Access**
```
GET  /api/maps                    # List all tested maps
GET  /api/maps/{id}/stats         # Get statistics for specific map
GET  /api/maps/{id}/bugs          # Retrieve bug reports with LLM descriptions
POST /api/maps/{id}/test          # Trigger new test run
GET  /api/compare?maps=id1,id2    # Compare two maps side-by-side
GET  /api/timeline/{run_id}       # Get frame-by-frame event timeline
```

**Use Case:** Integration with existing game dev tools (Unity Editor plugins, Unreal automation), external dashboards

**3. Web Dashboard - Visual Interface**

**Landing Page:**
- Map library (grid view with thumbnails, hardness scores, solvability badges)
- Quick stats: Total maps tested, average difficulty, bug detection rate
- Recent test runs timeline

**Map Detail Page:**
- Overview: Hardness score (0-100 gauge), solvability indicator, test run count
- Bug Report Section: Filterable list with:
  - Natural language LLM description (e.g., "Agent clipped through floor geometry at coordinates (234, 567)")
  - Frame screenshot with bounding box overlay
  - Severity badge (Critical/Major/Minor)
  - Reproduction steps
- Timeline Visualization: Interactive playback with synchronized video, telemetry graphs (health, ammo), event markers
- Comparative Analysis: Radar chart comparing difficulty dimensions (deaths, time, health management, navigation)

**Use Case:** QA teams reviewing test results, level designers iterating on maps, stakeholders viewing progress

### Interaction Workflow Example

**Scenario:** Level designer tests custom Doom map "arena_final.wad"

1. **Upload & Trigger Test (CLI):**
   ```bash
   python test_runner.py --map arena_final.wad --episodes 5
   ```
   Output: "Test run #456 started. ETA: 6 minutes."

2. **Automated Processing (Backend):**
   - Agent plays 5 episodes (2100 frames each)
   - Telemetry extracted in real-time (Member 2's module)
   - CV model analyzes 10,500 frames (Member 3's module)
   - 3 bugs detected → LLM generates descriptions (Member 3's module)
   - Results inserted into PostgreSQL (Member 1's module)

3. **Review Results (Web Dashboard):**
   - Navigate to `http://localhost:5000/maps/arena_final`
   - View hardness score: **67.5/100** (Hard difficulty)
   - See bug report: "Critical - Agent stuck at coordinates (1200, 450) due to missing collision bounds on platform mesh. Detected at frame 4959 (00:02:27)."
   - Click screenshot → see exact frame with bounding box (Member 4's UI)
   - Watch timeline playback → observe stuck behavior in context (Member 4's visualization)

4. **Fix & Retest:**
   - Designer fixes collision bounds in map editor
   - Re-run test: `python test_runner.py --map arena_final_v2.wad`
   - Compare results: `GET /api/compare?maps=arena_final,arena_final_v2`
   - Confirm bug resolved: Stuck events reduced from 3 → 0

### Accessibility & Integration

**External Tool Integration:**
- **Jira/GitHub Issues:** Export bug reports as JSON, auto-create tickets via API webhooks (Member 5)
- **Slack/Discord Notifications:** Post test completion alerts with quick stats (Member 5)
- **CI/CD Pipelines:** Jenkins/GitLab CI jobs trigger tests on map commits, fail builds if critical bugs detected (Member 5)

**Documentation:**
- API Reference: OpenAPI/Swagger spec hosted at `/api/docs` (Member 1)
- User Guide: Step-by-step tutorials with screenshots (Member 5)
- Architecture Docs: Component diagrams, database schema, extension guidelines (Member 2, Member 5)

---

This comprehensive planning section demonstrates a realistic, well-resourced project timeline with clear team responsibilities, transparent budget allocation, and practical user interaction models suitable for academic evaluation and industry consideration.

# 1.6 Project Planning and Management

This section outlines the implementation timeline, resource allocation, and budget planning for the automated game level testing framework. The project follows an iterative development approach with clearly defined milestones and deliverables.

## 1.6.1 Project Timeline Revisited

### Timeline Overview
The project is structured as a **12-week (3-month) implementation cycle** following the documentation phase, divided into four major phases: Foundation (Weeks 1-3), Core Backend Development (Weeks 4-7), Frontend & Integration (Weeks 8-10), and Testing & Finalization (Weeks 11-12).

### Team Structure (5 Members)

| Role | Team Member | Primary Responsibilities |
|------|-------------|--------------------------|
| **Team Lead & Backend Developer** | Member 1 | Database architecture, REST API development, integration coordination |
| **RL/ML Engineer** | Member 2 | Agent integration, telemetry extraction, reproducibility testing |
| **AI Engineer (CV/LLM)** | Member 3 | Computer Vision model training, LLM integration, bug detection algorithms |
| **Frontend Developer** | Member 4 | Web dashboard (starts Week 7), data visualization components, UI/UX design |
| **QA & DevOps Engineer** | Member 5 | Testing pipeline, deployment automation, documentation, performance optimization |

### Phase Breakdown (Day-by-Day Schedule)

---

## Phase 1: Foundation & Setup (Weeks 1-3, Days 1-21)

### Week 1 (Days 1-7)

**Day 1-2: Environment Setup & Project Initialization**
- Member 5: Install VizDoom, PyTorch, PostgreSQL on all machines
- Member 5: Setup Git repository, branching strategy, CI/CD skeleton
- Member 1: Initialize database project structure (migrations, ORM setup)
- Member 2: Clone pre-trained DQN agent repository, verify dependencies
- Member 3: Research CV architectures (ResNet, EfficientNet comparison)
- Member 4: Research frontend stack options (React vs Vue), create decision document
- **Deliverable:** All development environments operational, repo structure created

**Day 3-4: Database Schema Implementation**
- Member 1: Design and implement core tables (maps, runs, events, metrics)
- Member 1: Add foreign key constraints, indexes on frequently queried columns
- Member 1: Write migration scripts and seed data for testing
- Member 2: Write test script to load agent model and initialize VizDoom
- Member 3: Begin annotating training dataset (100 frames, 5 bug categories)
- Member 5: Setup PostgreSQL backup automation
- **Deliverable:** Database schema complete with migration scripts, agent loads successfully

**Day 5-7: Basic Test Runner Prototype**
- Member 2: Implement single-map test runner (load map, run agent, capture outcome)
- Member 2: Add seed control for reproducibility
- Member 1: Create database insertion functions for test runs
- Member 3: Continue dataset annotation (200 frames completed)
- Member 4: Study existing map visualization libraries (Plotly, D3.js)
- Member 5: Write unit tests for database models
- **Milestone (Day 7):** Agent successfully completes 1 map, basic data stored in database

---

### Week 2 (Days 8-14)

**Day 8-10: Telemetry Extraction Module**
- Member 2: Implement frame-by-frame data capture (health, ammo, position, events)
- Member 2: Create event logging system with timestamp precision
- Member 1: Optimize database insertion (batch inserts, transaction handling)
- Member 3: Finish dataset annotation (500 frames, 7 bug categories completed)
- Member 5: Setup automated testing pipeline (pytest integration)
- **Deliverable:** 10,500 events captured per 5-episode run, stored in <2 seconds

**Day 11-14: Reproducibility Validation & Data Pipeline**
- Member 2: Run reproducibility tests (5 identical-seed runs, measure variance)
- Member 2: Implement crash detection and graceful error handling
- Member 1: Add database query functions (get_map_stats, get_run_events)
- Member 3: Setup CV training environment (GPU instance, data loaders)
- Member 5: Write integration tests (end-to-end test run)
- Member 5: Create performance monitoring dashboard (query times, insert rates)
- **Milestone (Day 14):** <10% variance across runs verified, complete pipeline operational

---

### Week 3 (Days 15-21)

**Day 15-17: Bug Detection Algorithms (Behavioral)**
- Member 2: Implement stuck state detection (position unchanged 100+ frames)
- Member 2: Implement instant death detection (health drop >80 in 1 frame)
- Member 2: Implement unreachable area detection (coverage heatmap analysis)
- Member 3: Begin CV model training (transfer learning from ResNet-50)
- Member 1: Create bug_reports table schema
- Member 5: Write unit tests for bug detection logic
- **Deliverable:** Behavioral bug detection operational, 90%+ accuracy on test cases

**Day 18-21: Computer Vision Setup**
- Member 3: Fine-tune CV model on annotated dataset (500 frames)
- Member 3: Implement frame analysis pipeline (30 FPS processing)
- Member 3: Create visual_anomalies table integration
- Member 2: Test CV model on 5 diverse maps
- Member 1: Add API endpoint stubs (maps, runs, bugs - no implementation yet)
- Member 5: Setup GPU monitoring (utilization, memory tracking)
- **Milestone (Day 21):** CV model achieves 70%+ precision, integrated with test pipeline

---

## Phase 2: Core Backend Development (Weeks 4-7, Days 22-49)

### Week 4 (Days 22-28)

**Day 22-24: LLM Integration Foundation**
- Member 3: Setup LLM API clients (OpenAI GPT-4V, Anthropic Claude)
- Member 3: Implement prompt engineering for bug descriptions
- Member 3: Create smart sampling logic (invoke only when bugs detected)
- Member 1: Implement REST API core structure (Flask/FastAPI routes)
- Member 2: Create batch testing script (process multiple maps sequentially)
- **Deliverable:** LLM generates natural language bug reports with context

**Day 25-28: Hardness Scoring & Solvability**
- Member 2: Implement hardness score algorithm (weighted formula 0-100)
- Member 2: Implement solvability detection logic
- Member 2: Add hardness_score and is_solvable columns to metrics table
- Member 3: Optimize LLM prompts (reduce token usage by 30%)
- Member 1: Implement API endpoints: GET /api/maps, GET /api/maps/{id}/stats
- Member 5: Load testing (simulate 100 concurrent API requests)
- **Milestone (Day 28):** Hardness scoring validated on 10 maps, API returns statistics

---

### Week 5 (Days 29-35)

**Day 29-31: Advanced API Endpoints**
- Member 1: Implement GET /api/maps/{id}/bugs (return LLM descriptions with screenshots)
- Member 1: Implement GET /api/compare?maps=id1,id2 (comparative analysis)
- Member 1: Add caching layer (Redis) for frequently accessed queries
- Member 3: Train CV model on expanded dataset (750 frames)
- Member 2: Implement parallel execution support (4 concurrent test runs)
- **Deliverable:** 5 core API endpoints operational with <500ms response time

**Day 32-35: Query Optimization & Analytics**
- Member 1: Implement GET /api/timeline/{run_id} (frame-by-frame event stream)
- Member 1: Add database indexing for performance (map_id, run_id, event_type)
- Member 1: Create aggregate statistics endpoint (total maps, avg hardness, bug distribution)
- Member 3: Validate CV + LLM hybrid system on 20 maps
- Member 5: Write API integration tests (test all endpoints with mock data)
- **Milestone (Day 35):** 85%+ bug detection precision achieved, API complete

---

### Week 6 (Days 36-42)

**Day 36-38: Batch Testing Pipeline**
- Member 2: Implement batch testing CLI (process directory of maps)
- Member 2: Add progress tracking and ETA estimation
- Member 2: Implement test scheduling (queue system for large batches)
- Member 1: Add POST /api/maps/{id}/test endpoint (trigger test via API)
- Member 3: Create PDF report generation module (bug summaries)
- Member 5: Setup monitoring alerts (email notifications on failures)
- **Deliverable:** Batch testing processes 10+ maps unattended

**Day 39-42: Performance Optimization**
- Member 1: Optimize database queries (reduce N+1 queries, add eager loading)
- Member 1: Implement pagination for large result sets
- Member 2: Profile agent execution (identify bottlenecks)
- Member 3: Optimize CV model inference (batch frame processing)
- Member 5: Run performance benchmarks (test 50 maps, measure metrics)
- **Milestone (Day 42):** System tests 20+ maps in <2 hours, all backend complete

---

### Week 7 (Days 43-49)

**Day 43-45: API Documentation & Stabilization**
- Member 1: Generate OpenAPI/Swagger documentation
- Member 1: Add API authentication (optional JWT tokens)
- Member 1: Write API usage examples and tutorials
- Member 2: Fix edge cases (very large maps, corrupt files, timeouts)
- Member 3: Create bug report templates (JSON, PDF formats)
- Member 5: Security audit (SQL injection prevention, input validation)
- **Deliverable:** Complete API documentation with examples

**Day 46-49: Backend Integration Testing**
- Member 5: End-to-end testing (30+ diverse maps)
- Member 5: Stress testing (100 maps, measure system limits)
- Member 2: Bug fixes from integration testing
- Member 3: Fine-tune LLM prompts based on testing feedback
- Member 1: Database migration for production schema
- **Member 4: BEGINS FRONTEND WORK** (project scaffolding, initial setup)
- **Milestone (Day 49):** Backend 100% complete, 30+ maps tested successfully

---

## Phase 3: Frontend & Integration (Weeks 8-10, Days 50-70)

### Week 8 (Days 50-56)

**Day 50-52: Frontend Project Setup**
- Member 4: Initialize React.js project (Create React App or Vite)
- Member 4: Setup routing (React Router), state management (Redux/Context)
- Member 4: Create project structure (components, pages, services)
- Member 4: Implement API client (Axios with base URL config)
- Member 1: Add CORS headers to API for local frontend development
- Member 5: Setup frontend CI/CD pipeline (build, lint, deploy)
- **Deliverable:** Frontend dev server running, API connection verified

**Day 53-56: Landing Page & Map Library**
- Member 4: Design and implement landing page (hero section, quick stats)
- Member 4: Create map library grid view (thumbnails, hardness badges)
- Member 4: Implement map list API integration (fetch and display maps)
- Member 4: Add search and filter functionality (by difficulty, solvability)
- Member 1: Optimize map list endpoint (add pagination, thumbnails)
- Member 5: Frontend testing setup (Jest, React Testing Library)
- **Deliverable:** Map library page operational with filtering

---

### Week 9 (Days 57-63)

**Day 57-59: Map Detail Page (Part 1)**
- Member 4: Create map detail page layout (overview section)
- Member 4: Display hardness score (gauge/dial visualization)
- Member 4: Show solvability indicator and basic stats (runs, avg time)
- Member 4: Implement navigation from map list to detail page
- Member 1: Add map thumbnail generation endpoint
- Member 2: Support Member 4 with test data generation
- **Deliverable:** Map detail page shows basic statistics

**Day 60-63: Bug Report Dashboard**
- Member 4: Design bug report list component (table with filters)
- Member 4: Display LLM-generated descriptions with severity badges
- Member 4: Add screenshot modal (click to view full frame)
- Member 4: Implement frame number links (jump to timeline)
- Member 3: Optimize screenshot delivery (compress images, lazy loading)
- Member 5: Frontend performance testing (Lighthouse audit)
- **Deliverable:** Bug reports displayed with natural language descriptions

---

### Week 10 (Days 64-70)

**Day 64-66: Timeline Visualization (Part 1)**
- Member 4: Create timeline component (Plotly.js/D3.js)
- Member 4: Display telemetry graphs (health, ammo over time)
- Member 4: Add event markers (deaths, pickups) on timeline
- Member 4: Implement zoom and pan controls
- Member 1: Optimize timeline API (reduce payload size)
- **Deliverable:** Interactive timeline shows telemetry data

**Day 67-70: Timeline Visualization (Part 2) & Comparative View**
- Member 4: Add video playback synchronized with timeline
- Member 4: Overlay bug markers with LLM descriptions on timeline
- Member 4: Create comparative analysis page (side-by-side maps)
- Member 4: Implement radar chart for difficulty dimensions
- Member 1: Support Member 4 with compare API troubleshooting
- Member 5: Cross-browser testing (Chrome, Firefox, Safari)
- **Milestone (Day 70):** Timeline playback operational, comparison view complete

---

## Phase 4: Testing & Finalization (Weeks 11-12, Days 71-84)

### Week 11 (Days 71-77)

**Day 71-73: Frontend-Backend Integration**
- Member 1 & Member 4: Pair programming for integration issues
- Member 4: Fix API request errors, loading states, error handling
- Member 4: Add loading spinners and skeleton screens
- Member 4: Implement toast notifications for user feedback
- Member 5: End-to-end testing (user flows from map upload to report)
- **Deliverable:** Complete user workflows tested and functional

**Day 74-77: Advanced Visualizations & Polish**
- Member 4: Create bug occurrence heatmaps
- Member 4: Add analytics dashboard (aggregate statistics, charts)
- Member 4: Implement dark mode toggle
- Member 4: Polish UI (responsive design, mobile support)
- Member 3: Generate 50+ test reports for demo data
- Member 5: Accessibility audit (WCAG compliance check)
- **Deliverable:** Frontend polished with advanced visualizations

---

### Week 12 (Days 78-84)

**Day 78-80: System-Wide Testing**
- Member 5: User acceptance testing (simulate real-world workflows)
- Member 5: Performance testing (50+ maps, measure frontend load times)
- Member 2: Bug fixes from UAT
- Member 4: Frontend bug fixes (UI inconsistencies, edge cases)
- Member 1: Database optimization (vacuum, reindex)
- **Deliverable:** All critical bugs fixed, system stable

**Day 81-82: Documentation Finalization**
- Member 5: Write user guide (step-by-step tutorials with screenshots)
- Member 5: Write deployment guide (Docker setup, production config)
- Member 2: Update architecture documentation (add diagrams)
- Member 1: Finalize API documentation with examples
- Member 4: Create frontend component documentation
- **Deliverable:** Complete documentation package

**Day 83-84: Presentation & Release Preparation**
- All Members: Prepare final presentation (slides, demo script)
- All Members: Record demo video (5-minute walkthrough)
- Member 1: Create GitHub release (tag version, release notes)
- Member 5: Deploy to production environment (cloud hosting)
- All Members: Final rehearsal and Q&A preparation
- **Milestone (Day 84):** PROJECT COMPLETE - System operational, documented, and ready for demonstration

---

## Timeline Summary by Phase

| Phase | Duration | Key Focus | Team Members |
|-------|----------|-----------|--------------|
| **Phase 1: Foundation** | Days 1-21 (3 weeks) | Database, Agent, CV Training | Member 1, 2, 3, 5 |
| **Phase 2: Core Backend** | Days 22-49 (4 weeks) | API, LLM, Bug Detection, Testing | Member 1, 2, 3, 5 |
| **Phase 3: Frontend** | Days 50-70 (3 weeks) | Dashboard, Visualization, Integration | Member 1, 4, 5 (support) |
| **Phase 4: Finalization** | Days 71-84 (2 weeks) | Testing, Documentation, Presentation | All Members |

**Frontend Schedule:** Member 4 starts Day 46-49 (project setup during backend stabilization), works full-time Days 50-84 (5.5 weeks total)

### Task Dependencies

Critical path dependencies:
1. **Database Schema → Telemetry Extraction → API Development**
   - Database must exist before data can be stored (Member 1 → Member 2 → Member 1)
   - API queries require populated database tables

2. **Agent Integration → Bug Detection → LLM Integration**
   - Agent must generate gameplay data before bugs can be detected (Member 2 → Member 3)
   - LLM requires both telemetry and CV outputs for context (Member 3)

3. **Core Backend → Frontend Dashboard**
   - REST API must be operational before frontend can query data (Member 1 → Member 4)
   - Visualization depends on database aggregation functions

4. **All Components → Integration Testing**
   - QA testing blocked until all modules operational (Member 5 depends on all)

### GANTT Chart (12-Week Timeline)

```
┌──────────────────────────┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┐
│ Task                     │W1 │W2 │W3 │W4 │W5 │W6 │W7 │W8 │W9 │W10 │W11 │W12 │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Environment Setup (M5)   │███│   │   │   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Database Impl (M1)       │███│███│   │   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Agent Integration (M2)   │███│███│███│   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Telemetry Extract (M2)   │   │███│███│   │   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ CV Bug Detection (M3)    │   │   │███│███│   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ LLM Integration (M3)     │   │   │   │███│███│   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Hardness Score (M2)      │   │   │   │███│   │   │   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ REST API Dev (M1)        │   │   │   │███│███│███│   │   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Batch Testing (M2)       │   │   │   │   │   │███│███│   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Frontend Setup (M4)      │   │   │   │   │   │   │███│   │   │    │    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Frontend UI (M4)         │   │   │   │   │   │   │   │███│███│ ███│    │    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Visualization (M4)       │   │   │   │   │   │   │   │   │███│ ███│ ███│    │
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Integration Test (M5)    │   │   │   │   │   │   │   │   │   │    │ ███│ ███│
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Documentation (All)      │   │   │   │   │   │   │   │   │   │    │ ███│ ███│
├──────────────────────────┼───┼───┼───┼───┼───┼───┼───┼───┼───┼────┼────┼────┤
│ Presentation Prep (All)  │   │   │   │   │   │   │   │   │   │    │    │ ███│
└──────────────────────────┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┘

Legend: ███ = Active development phase
        M1-M5 = Team members responsible
        
Key Dates:
- Day 1-21 (W1-W3): Backend foundation
- Day 22-49 (W4-W7): Core backend development
- Day 46-49 (W7): Frontend starts (project setup)
- Day 50-70 (W8-W10): Frontend full development
- Day 71-84 (W11-W12): Testing and finalization
```

### Recommended Tools for Project Management

**GANTT Chart Creation:**

1. **GanttProject** (FREE - Highly Recommended for Students)
   - **Pros:** 
     - Completely free and open-source
     - Desktop application (Windows/Mac/Linux)
     - Professional GANTT charts with task dependencies
     - Export to PNG, PDF, HTML
     - Easy to use, no learning curve
     - **Perfect for 12-week day-by-day schedules**
   - **Cons:** 
     - No cloud sync (local files only)
     - Basic compared to enterprise tools
   - **Best For:** Graduation projects, detailed day-by-day planning
   - **Download:** ganttproject.biz

2. **Jira (Recommended for daily task management)**
   - **Pros:** 
     - Industry-standard for software projects
     - Built-in Agile/Scrum boards with sprint planning
     - Timeline view for week-by-week visualization
     - Issue tracking integrated with timeline
     - Free for teams up to 10 users
     - **Great for tracking daily tasks across 5 team members**
   - **Cons:** 
     - True GANTT requires Jira Premium ($7.75/user/month)
     - Steeper learning curve
   - **Best For:** Day-to-day sprint management, task assignments

3. **Monday.com**
   - **Pros:**
     - Intuitive drag-and-drop GANTT chart builder
     - Real-time collaboration
     - Beautiful visualizations
     - Timeline view with day-level granularity
   - **Cons:**
     - Paid only (starts at $8/user/month = $40/month for 5 users)
   - **Best For:** Teams with budget for premium tools

4. **Notion** (Alternative for Combined Planning)
   - **Pros:**
     - Free for students (verification required)
     - Combines documentation + task tracking + timeline
     - Day-by-day task tracking with calendar view
     - Collaborative workspace
   - **Cons:**
     - No native GANTT (requires templates or integrations)
   - **Best For:** All-in-one documentation and task management

5. **Mermaid Gantt Diagrams** (For Documentation)
   - **Pros:**
     - Code-based (version controllable)
     - Renders in Markdown files
     - Free, integrates with VS Code, GitHub
   - **Cons:**
     - Static (not interactive)
     - Limited day-level granularity
   - **Best For:** Embedding GANTT in documentation

**Our Recommendation for Your 5-Person, 12-Week, Day-by-Day Project:**
- **GANTT Chart for Documentation:** GanttProject (export professional PNG/PDF for Word document)
- **Daily Task Management:** Jira free tier (create tasks per day, assign to members, track in sprints)
- **Communication:** Slack/Discord free tier (daily standups, announcements)
- **Documentation:** Notion free student plan or Google Docs

**Setup Workflow:**
1. **Day 1:** Create 84 tasks in GanttProject (one per day) with dependencies
2. **Day 1:** Setup Jira board with 12 sprints (one per week)
3. **Weekly:** Break down weekly tasks into daily tickets in Jira
4. **Daily:** Team updates Jira task status (To Do → In Progress → Done)
5. **Weekly Review:** Update GanttProject milestones based on actual progress

### Risk Mitigation

| Risk | Probability | Impact | Mitigation Strategy | Owner |
|------|-------------|--------|---------------------|-------|
| CV model training takes longer than expected | Medium | High | Start with transfer learning; Member 3 begins Week 1, not Week 3 | Member 3 |
| LLM API costs exceed budget | Low | Medium | Implement smart sampling; Member 3 adds caching layer | Member 3 |
| GPU availability constraints | Medium | Medium | Reserve cloud GPU early; Member 5 coordinates access | Member 5 |
| Database query performance degrades | Low | Medium | Member 1 implements indexing from day 1; weekly perf reviews | Member 1 |
| Agent fails on unconventional maps | High | Low | Test diverse maps early (Week 2); document limitations | Member 2 |
| Frontend-backend integration conflicts | Medium | Medium | Daily standups; Member 1 & 4 pair programming in Week 6 | Member 1, 4 |
| Team member availability issues | Medium | High | Cross-training: each member understands 2+ components; documented handoff procedures | All |

### Communication Plan

**Daily Standups (15 minutes):**
- What did you complete yesterday?
- What are you working on today?
- Any blockers?

**Weekly Sprint Reviews (Friday, 1 hour):**
- Demo completed features
- Review milestone progress
- Plan next week's tasks

**Tools:**
- Slack/Discord: Daily communication
- Jira: Task tracking and sprint planning
- GitHub: Code reviews, pull requests
- Google Drive: Shared documentation

### Deviation Reporting Process

**If timeline slips:**
1. Identify blocking task and root cause (responsible member documents in Jira)
2. Team lead escalates to supervisor if milestone delivery shifts by >3 days
3. Hold emergency sprint planning to redistribute work
4. Update GANTT chart and communicate new ETA to all stakeholders

**Scope Adjustment Protocol:**
- **Must-Have (MVP):** Agent integration, telemetry extraction, database storage, basic bug detection, 2+ API endpoints
- **Should-Have:** CV module, LLM integration, hardness scoring, frontend dashboard
- **Could-Have:** PDF reports, advanced visualizations, batch testing UI

If time-constrained, drop features in reverse order (could-have → should-have), preserving MVP at all costs.

---

## 1.6.2 Preliminary Budget Adjusted

### Budget Overview

Total estimated cost for 3-month development cycle: **$550-750 USD** (5-person team)

### Itemized Cost Breakdown

| Category | Item | Unit Cost | Quantity | Total | Justification |
|----------|------|-----------|----------|-------|---------------|
| **Hardware** | | | | | |
| | GPU Server (Cloud) | $0.50/hour | 200 hours | $100 | CV model training (60h) + parallel testing runs (140h); AWS g4dn.xlarge spot instances over 3 months |
| | Local Development Machines | $0 | 5 | $0 | Personal laptops (existing); each 16GB RAM, CPU inference sufficient |
| **Software & APIs** | | | | | |
| | LLM API (GPT-4V/Claude) | $0.15/test | 200 tests | $30 | Smart sampling: 10-20 API calls per test × 200 maps (extended testing over 3 months) |
| | PostgreSQL Hosting | $25/month | 3 months | $75 | Managed database (DigitalOcean); 25GB storage, 500 connections |
| | VizDoom | $0 | - | $0 | Open-source (MIT license) |
| | PyTorch | $0 | - | $0 | Open-source (BSD license) |
| | Pre-trained DQN Agent | $0 | - | $0 | Existing models from research repository |
| **Project Management** | | | | | |
| | Jira (Free Tier) | $0 | 5 users | $0 | Up to 10 users free; sufficient for team |
| | GanttProject | $0 | - | $0 | Open-source desktop application |
| **Data & Resources** | | | | | |
| | Training Dataset (CV) | $0 | 750 frames | $0 | Self-generated; manual annotation distributed across team (15h × 5 members = 75h labor) |
| | Test Maps | $0 | 50+ maps | $0 | Community-created Doom maps (freely available) |
| **Development Tools** | | | | | |
| | Flask/FastAPI | $0 | - | $0 | Open-source Python frameworks |
| | React.js | $0 | - | $0 | Open-source frontend library |
| | Git/GitHub | $0 | - | $0 | Free tier (public repository, 5 collaborators) |
| | IDE (VS Code/PyCharm) | $0 | - | $0 | Free community editions |
| | draw.io | $0 | - | $0 | Free online diagramming tool |
| **Collaboration** | | | | | |
| | Slack/Discord | $0 | - | $0 | Free tier (unlimited messages) |
| | Google Workspace | $0 | - | $0 | Free tier (15GB storage per member) |
| | Zoom (Meetings) | $0 | - | $0 | Free tier (40-min meetings) |
| **Optional** | | | | | |
| | Domain Name | $12/year | 1 year | $12 | Optional for public demo (e.g., rlgametester.dev) |
| | SSL Certificate | $0 | - | $0 | Let's Encrypt (free) |
| | Monitoring (Grafana Cloud) | $0 | - | $0 | Free tier (10k metrics/month) |
| **Contingency** | | | | | |
| | Buffer for Overages | - | - | $333-533 | LLM costs if >200 tests, extra GPU hours, infrastructure needs, team collaboration tools |
| **TOTAL** | | | | **$550-750** | |

### Cost per Team Member
- **Total Budget:** $550-750
- **Per Member:** $110-150 per person (over 3 months)
- **Monthly:** ~$37-50 per person

### Cost Optimization Strategies (3-Month Timeline)

1. **GPU Usage:**
   - Use spot instances (70% cheaper than on-demand)
   - Member 3 schedules training during off-peak hours (nights/weekends)
   - Consolidate training sessions: Week 2-3 (CV training), Week 4-5 (fine-tuning)
   - Team shares GPU quota (sequential, not parallel training)
   - Target: <$120 GPU costs over 3 months

2. **LLM API Costs:**
   - Invoke only when CV detects bugs (85-90% cost reduction)
   - Member 3 implements aggressive caching for repeated bug patterns
   - Use smaller context windows (50-100 frames vs full episode)
   - Team agrees on test budget: max 200 maps total
   - Front-load testing: Weeks 5-7 (backend complete), minimize redundant tests
   - Target: $0.10-0.15 per test run

3. **Database Hosting:**
   - Start with free tier (Supabase: 0.5GB, Heroku: 1GB) for Weeks 1-6
   - Member 1 monitors storage usage weekly
   - Migrate to paid tier ($25/month) only after 500+ test runs (Week 7+)
   - Use connection pooling to delay paid tier migration
   - Potential savings: $50 (if free tier sufficient for first 2 months)

4. **Alternative Budget (Zero-Cost Approach):**
   - If team member has local GPU: $0 GPU costs
   - Local PostgreSQL: $0
   - No LLM integration (CV-only bug detection): $0
   - Total: **$0-50** (trade-off: reduced bug report quality, longer training time)

### Resource Requirements Summary

**Human Resources:**
- 5 Developers (graduation project team)
- Estimated effort per member: 112-126 hours over 12 weeks (9.3-10.5 hours/week each)
- Total team effort: 560-630 person-hours
- **Peak workload weeks:**
  - Week 7-10: Frontend development (Member 4: 30-35 hours/week)
  - Week 11-12: Integration testing and documentation (All members: 15-20 hours/week)

**Computational Resources:**
- Development: 4-core CPU, 16GB RAM per member (5 local machines)
- Training: NVIDIA T4 GPU equivalent (200 hours cloud time shared over 3 months)
- Production: 2-core CPU, 8GB RAM, 100GB SSD (sufficient for 1500 test runs)

**Data Storage:**
- Development: 20GB (database + video recordings + screenshots, distributed)
- Production: 100GB (500 maps × 5 runs × 10MB/run + 50GB overhead)

### Comparison to Manual QA Costs

**Manual Testing Baseline:**
- QA Tester Salary: $20-30/hour
- Testing Speed: 0.5 maps/hour (30-60 minutes per map)
- Cost per Map: $40-60
- Cost for 100 Maps: **$4,000-6,000**

**Automated Framework:**
- Setup Cost: $400-550 (one-time, team development)
- Testing Speed: 10 maps/hour (6 minutes per map)
- Cost per Map: $0.15 (LLM) + $0.50 (GPU) = $0.65
- Cost for 100 Maps: **$65** (after setup)

**ROI Calculation:**
- Breakpoint: 7-9 maps tested (setup cost recovered)
- Savings at 100 Maps: $3,550-5,550 (90-95% cost reduction)
- Time Savings: 50 hours vs 5 hours (10× speedup)

### Budget Approval & Contingency

**Approved By:** Prof./Dr. Mohamed Taher (pending)  
**Funding Source:** Personal funding (team-sponsored: $110-150 per member over 3 months)  
**Contingency Protocol:** If budget exceeds $750, reduce LLM tests to 150 maps or eliminate LLM integration entirely (fallback to CV-only detection)

**Cost Sharing Agreement:**
- Each team member contributes equally: $110-150 per person (over 3 months)
- Member 3 (AI Engineer) leads cost optimization given high GPU/LLM usage
- Team lead (Member 1) approves all infrastructure purchases
- Monthly check-ins to review spending (Day 30, Day 60)

---

## 1.6.3 User Interface & Interaction Model

### System Access Points

The framework provides three primary interfaces for user interaction:

**1. Command-Line Interface (CLI) - Primary Testing Interface**
```bash
# Run single map test
python test_runner.py --map maps/arena_x1.wad --episodes 5

# Batch testing
python test_runner.py --map-dir maps/ --parallel 4

# Query results
python query_api.py --map-id arena_x1 --output json
```

**Use Case:** Automated CI/CD integration, scripted testing pipelines, power users

**2. REST API - Programmatic Access**
```
GET  /api/maps                    # List all tested maps
GET  /api/maps/{id}/stats         # Get statistics for specific map
GET  /api/maps/{id}/bugs          # Retrieve bug reports with LLM descriptions
POST /api/maps/{id}/test          # Trigger new test run
GET  /api/compare?maps=id1,id2    # Compare two maps side-by-side
GET  /api/timeline/{run_id}       # Get frame-by-frame event timeline
```

**Use Case:** Integration with existing game dev tools (Unity Editor plugins, Unreal automation), external dashboards

**3. Web Dashboard - Visual Interface**

**Landing Page:**
- Map library (grid view with thumbnails, hardness scores, solvability badges)
- Quick stats: Total maps tested, average difficulty, bug detection rate
- Recent test runs timeline

**Map Detail Page:**
- Overview: Hardness score (0-100 gauge), solvability indicator, test run count
- Bug Report Section: Filterable list with:
  - Natural language LLM description (e.g., "Agent clipped through floor geometry at coordinates (234, 567)")
  - Frame screenshot with bounding box overlay
  - Severity badge (Critical/Major/Minor)
  - Reproduction steps
- Timeline Visualization: Interactive playback with synchronized video, telemetry graphs (health, ammo), event markers
- Comparative Analysis: Radar chart comparing difficulty dimensions (deaths, time, health management, navigation)

**Use Case:** QA teams reviewing test results, level designers iterating on maps, stakeholders viewing progress

### Interaction Workflow Example

**Scenario:** Level designer tests custom Doom map "arena_final.wad"

1. **Upload & Trigger Test (CLI):**
   ```bash
   python test_runner.py --map arena_final.wad --episodes 5
   ```
   Output: "Test run #456 started. ETA: 6 minutes."

2. **Automated Processing (Backend):**
   - Agent plays 5 episodes (2100 frames each)
   - Telemetry extracted in real-time (Member 2's module)
   - CV model analyzes 10,500 frames (Member 3's module)
   - 3 bugs detected → LLM generates descriptions (Member 3's module)
   - Results inserted into PostgreSQL (Member 1's module)

3. **Review Results (Web Dashboard):**
   - Navigate to `http://localhost:5000/maps/arena_final`
   - View hardness score: **67.5/100** (Hard difficulty)
   - See bug report: "Critical - Agent stuck at coordinates (1200, 450) due to missing collision bounds on platform mesh. Detected at frame 4959 (00:02:27)."
   - Click screenshot → see exact frame with bounding box (Member 4's UI)
   - Watch timeline playback → observe stuck behavior in context (Member 4's visualization)

4. **Fix & Retest:**
   - Designer fixes collision bounds in map editor
   - Re-run test: `python test_runner.py --map arena_final_v2.wad`
   - Compare results: `GET /api/compare?maps=arena_final,arena_final_v2`
   - Confirm bug resolved: Stuck events reduced from 3 → 0

### Accessibility & Integration

**External Tool Integration:**
- **Jira/GitHub Issues:** Export bug reports as JSON, auto-create tickets via API webhooks (Member 5)
- **Slack/Discord Notifications:** Post test completion alerts with quick stats (Member 5)
- **CI/CD Pipelines:** Jenkins/GitLab CI jobs trigger tests on map commits, fail builds if critical bugs detected (Member 5)

**Documentation:**
- API Reference: OpenAPI/Swagger spec hosted at `/api/docs` (Member 1)
- User Guide: Step-by-step tutorials with screenshots (Member 5)
- Architecture Docs: Component diagrams, database schema, extension guidelines (Member 2, Member 5)

---

This comprehensive planning section demonstrates a realistic, well-resourced project timeline with clear team responsibilities, transparent budget allocation, and practical user interaction models suitable for academic evaluation and industry consideration.

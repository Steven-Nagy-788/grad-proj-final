# 1.4 System Objectives

The primary goal of this system is to develop a generalizable, automated testing framework for procedurally generated or user-created game levels using reinforcement learning agents. The following SMART objectives define the measurable success criteria for the system:

## 1.4.1 Automate Level Testing Through RL Agent Deployment

**Specific**: Deploy pre-trained DQN agents to autonomously test game levels without human intervention, navigating level geometry and interacting with game elements.

**Measurable**: Successfully test at least 10 different map configurations with zero manual setup time per map.

**Achievable**: Leverage existing VizDoom API and PyTorch DQN implementations to handle varying map complexities (small/medium/large) through adaptive action selection.

**Relevant**: Addresses the core problem of manual testing bottlenecks in level design workflows, directly supporting faster iteration cycles.

**Time-bound**: Complete autonomous testing capability for 10 maps within the first 2 weeks of implementation phase.

---

## 1.4.2 Achieve 10x Speedup Over Manual Testing

**Specific**: Reduce level testing time from an average of 30-60 minutes per map (manual QA baseline) to 3-6 minutes per map through automated agent execution.

**Measurable**: Document baseline manual testing times for 5 reference maps, then compare against automated system completion times, targeting a 10x improvement ratio.

**Achievable**: RL agents operate at accelerated game speeds (3-5x real-time) and require no breaks, fatigue management, or task switching overhead.

**Relevant**: Demonstrates economic viability and ROI for small-to-medium studios with limited QA budgets, proving the system's practical value.

**Time-bound**: Achieve verified 10x speedup measurements for at least 5 maps by week 3 of testing phase.

---

## 1.4.3 Detect Common Level Design Bugs with 85% Precision

**Specific**: Implement hybrid AI detection system combining Computer Vision (CNN/ResNet) and LLM analysis to identify visual anomalies (z-fighting, texture bleeding, clipping) and behavioral issues (stuck states, unreachable areas).

**Measurable**: Achieve 85% precision rate on a labeled test set of 50 maps containing known bugs, with false positive rate below 15%.

**Achievable**: Leverage transfer learning from pre-trained CV models and fine-tune on game-specific visual features, supplemented by telemetry-based behavioral detection.

**Relevant**: Provides actionable feedback to level designers, directly reducing post-release bug patches and player complaints.

**Time-bound**: Reach 85% precision threshold by end of week 4 through iterative model training and threshold tuning.

---

## 1.4.4 Ensure Reproducible Testing with <10% Score Variance

**Specific**: Enable deterministic test execution by controlling random seeds, ensuring identical agent behavior across multiple runs for regression testing and A/B map comparisons.

**Measurable**: Execute 5 repeated test runs on the same map with identical seeds, measuring variance in agent performance scores (completion time, kill count, item pickups) to remain below 10%.

**Achievable**: VizDoom and PyTorch both support deterministic execution through seed control; eliminate non-deterministic operations in agent inference.

**Relevant**: Critical for validating bug fixes and objectively comparing map iterations, enabling data-driven design decisions.

**Time-bound**: Verify <10% variance across 3 different maps by week 2 of implementation.

---

## 1.4.5 Demonstrate 60% Code Reusability for Engine Portability

**Specific**: Architect the system with clear separation between engine-specific (VizDoom adapter, 39% of codebase) and engine-agnostic components (database, API, frontend, LLM integration, 61% of codebase).

**Measurable**: Perform static code analysis to verify that at least 60% of total codebase (measured in lines of code) is independent of VizDoom APIs and can be reused for other game engines.

**Achievable**: Apply adapter pattern to isolate engine-specific logic in dedicated modules, with well-defined interfaces for telemetry extraction and action injection.

**Relevant**: Positions the research as a generalizable methodology applicable to Unity, Unreal, Godot, and other engines, increasing academic and industry impact.

**Time-bound**: Document adapter implementation and achieve 60% reusability measurement by end of implementation phase (week 7).

---

## 1.4.6 Minimize Infrastructure Costs to <$0.20 Per Test

**Specific**: Optimize operational costs by using local GPU inference for Computer Vision detection (zero API costs) and invoking LLM APIs only when bugs are detected, rather than processing entire gameplay videos.

**Measurable**: Achieve per-test costs below $0.20, compared to LLM-only video analysis approaches costing $7-15 per test (85-90% cost reduction).

**Achievable**: Smart sampling strategy: CV model runs locally on every frame, LLM called only 10-20 times per test session (when bugs detected) instead of 1000+ frames.

**Relevant**: Makes automated testing economically accessible to indie studios, academic researchers, and small teams with limited budgets.

**Time-bound**: Validate cost projections through 20 test runs with API usage tracking by week 5.

---

## 1.4.7 Generate Structured Reports with 100% Coverage

**Specific**: Produce comprehensive testing reports integrating telemetry data (JSON), visual evidence (frame screenshots), and natural language bug descriptions (LLM-generated), exported in both JSON and PDF formats.

**Measurable**: Every detected bug includes: timestamp, frame screenshot, severity classification (critical/major/minor), reproduction steps, and LLM description (minimum 50 words). 100% of bugs in test set receive complete reports.

**Achievable**: Pipeline combines CV detection outputs, telemetry logs, and LLM API responses into structured templates using Python report generation libraries.

**Relevant**: Bridges the gap between automated detection and human decision-making, ensuring test results integrate seamlessly with existing issue tracking workflows (Jira, GitHub Issues).

**Time-bound**: Achieve 100% report coverage for all detected bugs by week 6, with PDF export functionality operational.

---

## Summary

These seven SMART objectives collectively define a system that is:
- **Fast**: 10x speedup over manual testing (3-6 minutes vs 30-60 minutes per map)
- **Accurate**: 85% bug detection precision with <10% score variance
- **Generalizable**: 60% code reusability across game engines
- **Cost-effective**: <$0.20 per test (85-90% cost reduction)
- **Actionable**: 100% report coverage with structured, human-readable outputs

Success will be validated through quantitative metrics (time, cost, accuracy, variance) measured within a 7-week implementation timeline, with weekly milestone checkpoints for progress tracking.
